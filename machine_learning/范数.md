## L0范数与L1范数

L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。

L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。

为什么L1范数会使权值稀疏？它是L0范数的最优凸近似。任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。

一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。

为什么要稀疏？

**1）特征选择(Feature Selection)**

**2）可解释性(Interpretability)**

只有少数特征是非零的，说明观测目标只与这几个特征有关

## L2范数

回归里，又称为岭回归（Ridge Regression），或权值衰减（weight decay）。

它主要用于改善过拟合问题。

L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。

L2范数的好处

**1）学习理论的角度：**

​       从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。

**2）优化计算的角度：**

​       从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。

**关于condition number**

优化有两大难题，一是：局部最小值，二是：ill-condition病态问题。

ill-condition对应的是well-condition。假设我们有个方程组AX=b，我们需要求解X。如果A或者b稍微的改变，会使得X的解发生很大的改变，那么这个方程组系统就是ill-condition的，反之就是well-condition的。

因为一般我们的系数矩阵A和b是从实验数据里面估计得到的，所以它是存在误差的，如果我们的系统对这个误差是可以容忍的就还好，但系统对这个误差太敏感了，以至于我们的解的误差更大，那这个解就太不靠谱了。所以这个方程组系统就是ill-conditioned病态的

condition number就是拿来衡量ill-condition系统的可信度的。condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。condition number值小的（1附近）就是well-conditioned的，大的（远大于1）就是ill-conditioned的。

如果方阵A是非奇异的，那么A的conditionnumber定义为矩阵A的norm乘以它的逆的norm。

如果方阵A是奇异的，那么A的condition number就是正无穷大了。

## 核范数

核范数||W||*是指矩阵奇异值的和，英文称呼叫Nuclear Norm。

作用：约束Low-Rank（低秩）

如果矩阵之间各行的相关性很强，那么就表示这个矩阵实际可以投影到更低维的线性子空间，也就是用几个向量就可以完全表达了，它就是低秩的。

如果矩阵表达的是结构性信息，例如图像、用户-推荐表等等，那么这个矩阵各行之间存在着一定的相关性，那这个矩阵一般就是低秩的。

因为rank()是非凸的，在优化问题里面很难求解，那么就需要寻找它的凸近似来近似它了。rank(w)的凸近似就是核范数||W||*。

低秩的应用：

矩阵填充，鲁棒PCA，背景建模，变换不变低秩纹理（TILT）





https://blog.csdn.net/zouxy09/article/details/24971995

https://blog.csdn.net/zouxy09/article/details/24972869