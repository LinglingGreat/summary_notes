# 线性回归

决策函数：$f(x) = w_1x_1 + w_2 x_2 + ... + w_d x_d + b = w^T x + b$

线性模型形式简单、易于建模，但却蕴涵着机器学习中一些重要的基本思想。许多功能更为强大的非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得。此外，由于w直观表达了各属性在预测中的重要性，因此线性模型有更好的可解释性。例如影响目标的因素有哪些，哪个因素最重要等。

考虑单调可微函数g(.)，令 $y=g^{-1}(w^Tx+b)$，这样的模型称为"广义线性模型"，其中函数g称为"联系函数"(link function).对数线性回归是广义线性模型在g(.)=ln(.)时的特例。

损失函数：squared error  $ err(\hat{y}, y) = (\hat{y}, y)^2 $

**最小化损失函数（参数学习/假设搜索）**

均方误差，对应了常用的欧几里得距离。基于均方误差最小化来进行模型求解的方法称为"最小二乘法"。

$$   (w^*,b^*) = \mathop{\arg\min}_{(w,b)} \sum\limits_{i=1}^{m}(f(x_i)-y_i)^2=arg min_{(w,b)} \sum\limits_{i=1}^{m}(y_i-wx_i-b)^2 $$

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear1.png)

矩阵为满秩矩阵或正定矩阵时，可逆；不可逆时，存在多个解，选择哪个由学习算法的归纳偏好决定，常见的做法是引入正则化项。

**基于梯度下降的参数学习/假设搜索**

梯度下降法：求解无约束优化问题最简单、最经典的方法之一

当目标函数为凸函数时，局部极小点对应着函数的全局最小点，此时梯度下降法可确保收敛到全局最优解



考虑无约束优化问题$min_x f(x)$, 其中f(x)为连续可微函数

若能构造一个序列$x^0, x^1, x^2,...满足f(x^{t+1}) < f(x^t), t=0,1,2,...$

则不断执行该过程即可收敛到局部极小点

根据泰勒展开式有$f(x+\Delta x) ~= f(x) + \Delta x^T \nabla f(x) $

于是，欲满足$f(x+\Delta x) < f(x)$，可选择 $ \Delta x = -\gamma\nabla f(x)$

其中步长$\gamma$ 是一个小常数。这就是梯度下降法



为了找到一个函数的局部极小值，于是让x朝着当前点对应梯度（或者是近似梯度）的反方向行走一定的距离进行迭代搜索

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear2.jpg)

批量梯度下降，随机梯度下降，小批量梯度下降



迭代终止：
定义一个合理的阈值，当两次迭代之间的差值小于该阈值时，迭代结束;
设置一个大概的迭代步数;
设置函数阈值；

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear3.png)

# 线性分类(逻辑回归)

如何使用线性模型进行回归学习？只需找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。



对数几率回归方法的优点：

1. 它是直接对分类可能性进行建模，无需实现假设数据分布，这样就避免了假设分布不准确所带来的问题；
2. 它不是仅预测出“类别”，而是可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用；
3. 对数函数是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear4.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear5.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear6.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear7.png) 

# 其他问题(从线性到非线性、从二分类到多分类、类别不平衡)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear8.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear9.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear10.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear11.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear12.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear13.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear14.png) 

## 类别不平衡问题

当不同类别的训练样例数目差别很大的时候，将对一些传统机器学习模型的训练造成困扰。
困扰何在？
假设我们现在的任务是判断交易欺诈。
历史交易中的欺诈记录却很少。
欺诈行为带来的损失又是巨大的。所以希望尽可能地将欺诈行为给判断出来。
假设我们的训练数据包含10,000个真实交易以及10个欺诈交易。那么用这个数据训练模型（找寻假设）的时候，模型将所有的交易行为判断为真实交易即能够达到超过99.9%的准确度。一个“好模型”?

如何解决？

假定正例样本很少，反例样本很多。

**欠采样**：
去除一些反例样本使得正、反数据接近。然后再进行学习。
**过采样（重采样）**：
增加一些正例样本使得正、反数据接近。然后再进行学习。
**代价敏感**：
增加将正例样本错判为负例样本的惩罚。或采用“阈值移动”的方法。

#过拟合

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear15.png) 

应该尽可能地学出适用于所有潜在样本的“普遍规律”

可能把训练样本自身的一些特点当做了所有潜在样本都会具有的一般性质，从而导致泛化能力下降。这种现象 在机器学习中称为“过拟合”(overfitting)

与“过拟合”相对的是“欠拟合”(underfitting)，这是指对训练样本的一般性质尚未学好

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear16.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear17.jpg) 

==**偏差-方差窘境(bias-variance dilemma)**==

偏差.
偏差度量了学习算法的期望预测与真实结果的偏离程序, 即 刻画了学习算法本身的拟合能力 .

方差.
方差度量了同样大小的训练集的变动所导致的学习性能的变化, 即 刻画了数据扰动所造成的影响 .

噪声.
噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界, 即 刻画了学习问题本身的难度 . 巧妇难为无米之炊, 给一堆很差的食材, 要想做出一顿美味, 肯定是很有难度的.

想当然地, 我们希望偏差与方差越小越好, 但实际并非如此. 一般来说, 偏差与方差是有冲突的, 称为偏差-方差窘境 (bias-variance dilemma).

假定我们能够控制学习算法的拟合能力

当学习器的拟合能力不够强时，训练数据的扰动不足以使得学习器产生显著变化，此时偏差主导了泛化错误率
当学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能够被学习器学到，方差逐渐主导了泛化错误率
当学习器的拟合能力非常强时，训练数据发生的轻微扰动都会使得学习器发生显著变化。若训练数据自身的、非全局的特性被学习器学到了，则发生了过拟合。

欠拟合则通常是由学习能力低造成的，增加模型的复杂度可以解决（e.g., 增加多项式的最高幂）

有多种因素可能导致过拟合，其中最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了
**过拟合是机器学习面临的关键障碍，研究重点。各类算法在提出之后，都有人在研究避免其过拟合的措施。**

==过拟合：增大数据规模、减小数据特征数（维数）、增大正则化系数λ
欠拟合：增多数据特征数、添加高次多项式特征、减小正则化系数λ==

特征数非常多时，用线性函数也可能导致过拟合。

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear18.png) 

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear19.png) 

L1正则化的求解可以使用近端梯度下降（Proximal Gradient Descent，PGD）
P. L. Combettesand V. R. Wajs, “Signal recovery by proximal forwardbackwardsplitting,” MultiscaleModeling and Simulation, vol. 4, no. 4, pp. 1168–1200, 2006.

结构风险

表达了我们希望获得具有何种性质的模型（例如希望获得复杂度较小的模型）。这是一种引入领域知识和用户意图的途径。另一方面，该信息有助于削减假设空间，从而降低了最小化训练误差的过拟合风险。

==防止过拟合的方法==

https://www.zhihu.com/question/59201590

- simpler model structure
- regularization
- data augmentation
- dropout
- Bootstrap/Bagging
- ensemble
- early stopping
- utilize invariance
- Bayesian

# 参数调节

本质就是模型选择：不同的参数对应不同的模型

如何构建候选模型（参数）集合：
将各参数变量值的可行区间（可从小到大），划分为一系列的小区
产生对应各参数变量值组合
计算各参数组合下的模型的性能
逐一比较择优，从而求得最佳参数组合。

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/linear20.png) 

#==线性分类器==

**以下()属于线性分类器最佳准则?**

```
感知准则函数
贝叶斯分类
支持向量机
Fisher准则
```

ACD，[赵子龙](https://www.nowcoder.com/profile/730218)

线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。

感知器准则函数：代价函数J=-(W*X+w0)，分类的准则是最小化代价函数。感知器是神经网络（NN）的基础，网上有很多介绍。

SVM：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题）

Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。

贝叶斯分类器：一种基于统计方法的分类器，要求先了解样本的分布特点（高斯、指数等），所以使用起来限制很多。在满足一些特定条件下，其优化目标与线性分类器有相同结构（同方差高斯分布等），其余条件下不是线性分类。

参考：http://blog.163.com/rustle_go_go/blog/static/20294501420122110431306/