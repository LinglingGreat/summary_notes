聚类 Clustering

# 什么是聚类

监督学习：发现数据属性和类别属性之间的关联模式。并通过利用这些模式用来预测未知数据实例的类别属性。

无监督学习：数据没有目标属性。发现数据中存在的内在结构。

聚类（Clustering）是一种发现数据中的相似群（聚类，clusters）的技术。

聚类是一个将数据集中在某些方面相似的数据成员进行分类组织的过程。

一个聚类就是一些数据实例的集合，这个集合中的元素彼此相似；与其他聚类中的元素不同。

**聚类的应用**

聚类既可以作为一个单独过程，用于找寻数据内在的分布结构。也可以作为分类等其他学习任务的前驱过程。

实例1：在营销学中，对客户进行分割，为每组客户指定一个套营销策略，也是采用聚类来完成。

实例2：在生物学上，聚类能用于推导植物和动物的分类，对基因进行分类，获得对种群中固有结构的认识。

事实上，聚类是数据挖掘技术中应用最广泛的技术之一。
发展历史长，应用领域广泛。比如：医学类、心理学、植物学、社会学、生物学、营销学。近年来，在线文档的快速发展，文本聚类研究成为关注的重点。对给定文本，需要根据它们内容的相似性来进行组织。建立一个主题层次。

**聚类的形式定义**

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster1.png)

**影响聚类算法效果的主要原因有：（　）？**

```
特征选取
模式相似性测度
分类准则
已知类别的样本质量
```

正确答案为A B  C。这道题的答案可以从网上找到。http://www.docin.com/p-756247716.html

D之所以不正确，是因为聚类是对无类别的数据进行聚类，不使用已经标记好的数据。

# 聚类算法概述

**聚类算法**

- 原型聚类
- 层次聚类
- 密度聚类

**距离函数**（相似性或相异性）：度量两个数据点（对象）的相似程度。
**聚类评价**

- 类内差异（聚类内部距离）：最小化
- 类间差异（聚类外部距离）：最大化

聚类结果的质量与**算法、距离函数和应用领域**有很大关系。

## K-均值算法

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster2.png)

选择正确的K值 https://byteacademy.co/blog/k-means-clustering/

弯管法  **Elbow Method** 

弯管法允许我们通过视觉辅助对 K 值做出判定。我们试着将我们的数据分解成不同数量的 K 簇，并根据相应的 W（Ck）画出每个 K 簇类型。

$W(C_k)=\frac1{|C_k|}\sum_{i,i'\in C_k}\sum_{j=1}^p{(x_{ij}-x_{i'j})}^2$

![K-means](https://byteacademy.co/wp-content/uploads/2018/07/Screen-Shot-2018-07-14-at-9.12.17-am.png) 

选择W(Ck)值下降最快的地方对应的K值，这里选择K=2

**Silhouette Method/Analysis** 

Silhouette Analysis可以用来学习结果簇之间的间隔距离。Silhouette coefficient度量了一个簇中的每个点和其邻居簇中的点有多接近，因此可以用来评估例如簇的个数这样的参数。取值范围[-1,1].如果这个相关系数接近1说明样本和邻居簇距离很远，0说明样本在两个相邻簇的决策边界上/很接近决策边界，负值说明样本可能分配到了错误的簇。

一个样本的Silhouette coefficient=(b-a)/max(a,b)，b是样本和最近的簇(不是样本所属簇)之间的距离，a是平均簇内距离/方差。

http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html

==**k-means中的邻近度函数**==

1、曼哈顿距离： 质心：中位数。目标函数：最小化对象到其簇质心的距离和

2、平方欧几里德距离。质心：均值。目标函数：最小化对象到其簇质心的距离的平方和

3、余弦。质心：均值。最大化对象与其质心的余弦相似度和

4、Bregman 散度。质心：均值。目标函数：最小化对象到其簇质心的Bregman散度和

==使用K均值聚类时需要注意的地方==

**1. 输入数据一般需要做缩放，如标准化。**

原因很简单，K均值是建立在距离度量上的，因此不同变量间如果维度差别过大，可能会造成少数变量“施加了过高的影响而造成垄断”。

**2. 如果输入数据的变量类型不同，部分是数值型（numerical），部分是分类变量（categorical），需要做特别处理。**

方法1是将分类变量转化为数值型，但缺点在于如果使用独热编码（one hot encoding）可能会导致数据维度大幅度上升，如果使用标签编码（label encoding）无法很好的处理数据中的顺序（order）。方法2是对于数值型变量和分类变量分开处理，并将结果结合起来，具体可以参考Python的实现[1]，如K-mode和K-prototype。

**3. 输出结果非固定，多次运行结果可能不同。**

首先要意识到K-means中是有随机性的，从初始化到收敛结果往往不同。一种看法是强行固定随机性，比如设定sklearn中的random state为固定值。另一种看法是，如果你的K均值结果总在大幅度变化，比如不同簇中的数据量在多次运行中变化很大，那么K均值不适合你的数据，不要试图稳定结果 [2]。

我个人倾向于后者的看法，K均值虽然易懂，但效果一般，如果多次运行的结果都不稳定，不建议使用K均值。

**4. 运行时间往往可以得到优化，选择最优的工具库。**

基本上现在的K均值实现都是K-means++，速度都不错。但当数据量过大时，依然可以使用其他方法，如MiniBatchKMeans [3]。上百万个数据点往往可以在数秒钟内完成聚类，推荐Sklearn的实现。

**5. 高维数据上的有效性有限。**

建立在距离度量上的算法一般都有类似的问题，那就是在高维空间中距离的意义有了变化，且并非所有维度都有意义。这种情况下，K均值的结果往往不好，而通过划分子空间的算法（sub-spacing method）效果可能更好。

**6. 运行效率与性能之间的取舍。**

但数据量上升到一定程度时，如>10万条数据，那么很多算法都不能使用。最近读到的一篇对比不同算法性能随数据量的变化很有意思 [4]。在作者的数据集上，当数据量超过一定程度时仅K均值和HDBSCAN可用。

## **总结**

因此不难看出，K均值算法最大的优点就是运行速度快，能够处理的数据量大，且易于理解。但缺点也很明显，就是算法性能有限，在高维上可能不是最佳选项。

一个比较粗浅的结论是，在数据量不大时，可以优先尝试其他算法。当数据量过大时，可以试试HDBSCAN。仅当数据量巨大，且无法降维或者降低数量时，再尝试使用K均值。

一个显著的问题信号是，如果多次运行K均值的结果都有很大差异，那么有很高的概率K均值不适合当前数据，要对结果谨慎的分析。

此外无监督聚类的评估往往不易，基本都是基于使用者的主观设计，如sklearn中提供的Silhouette Coefficient和 Calinski-Harabaz Index [5]。更多关于无监督学习如何评估可以参考 [6]。

------

[1] nicodv/kmodes

[2] Changes of clustering results after each time run in Python scikit-learn

[3] sklearn.cluster.MiniBatchKMeans - scikit-learn 0.19.1 documentation

[4] Benchmarking Performance and Scaling of Python Clustering Algorithms

[5] 2.3. Clustering - scikit-learn 0.19.1 documentation

[6] 微调：一个无监督学习算法，如何判断其好坏呢?

## 基于高斯混合分布的聚类

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster3.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster4.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster5.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster6.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster7.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster8.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster9.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster10.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster11.png)

2-12：基于EM算法对参数进行迭代更新

停止条件的设定可以是：1. 达到最大迭代论述，或似然函数LL(D)增长小于阈值

14-17：根据高斯混合分布确定簇划分

## 密度聚类

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster12.png)

### DBSCAN

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster13.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster14.png)

核心对象：如果对象的Eps邻域至少包含最小数目MinPts的对象，则称该对象为核心对象。
边界点：边界点不是核心点，但落在某个核心点的邻域内。
噪音点：既不是核心点，也不是边界点的任何点

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster15.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster16.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster17.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/cluster18.png)