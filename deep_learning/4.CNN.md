#Foundations of Convolutional Neural Networks 

##è®¡ç®—æœºè§†è§‰

Computer Vision Problems:

Image Classification

Object detection

Neural Style Transfer

## è¾¹ç¼˜æ£€æµ‹ç¤ºä¾‹

Vertical edge detection

## æ›´å¤šè¾¹ç¼˜æ£€æµ‹å†…å®¹

é€šè¿‡å·ç§¯æ“ä½œå¯ä»¥æ‰¾å‡ºæ°´å¹³æˆ–åž‚ç›´è¾¹ç¼˜(æ¯”å¦‚æ æ†)

![edgedetect](img/edgedetect.png)

![edgedetect1](img/edgedetect1.png)



Sober filter
$$
\begin{matrix}
        1 & 0 & -1 \\
        2 & 0 & -2 \\
        1 & 0 & -1 \\
        \end{matrix}
$$
Scharr filter
$$
\begin{matrix}
          3 & 0 & -3 \\
        10 & 0 & -10 \\
          3 & 0 & -3 \\
        \end{matrix}
$$
æˆ–è€…ä½¿ç”¨ç¥žç»ç½‘ç»œåå‘ä¼ æ’­å­¦ä¹ filter

## Padding

å·ç§¯åŽçš„ç»´åº¦æ˜¯(n-f+1)Ã—(n-f+1)

ç¼ºç‚¹ï¼š

1.æ¯æ¬¡åšå·ç§¯æ“ä½œæ—¶ï¼Œå›¾åƒéƒ½ä¼šç¼©å°

2.è§’è½è¾¹çš„åƒç´ ç‚¹è¾“å‡ºè¾ƒå°‘ï¼Œä¸¢æŽ‰äº†å¾ˆå¤šè¾¹ç¼˜ä¿¡æ¯



å››å‘¨å¡«å……padding, å¡«å……å±‚æ•°ä¸ºpè¾“å‡ºç»´åº¦å˜æˆ(n+2p-f+1)Ã—(n+2p-f+1)

Valid convolutionsï¼šnÃ—n  * fÃ—f  â€”â€”> (n-f+1)Ã—(n-f+1)

Same convolutionsï¼šPad so that output size is the same as the input size

(n+2p-f+1)Ã—(n+2p-f+1)ï¼Œä»¤n+2p-f+1=n, p=(f-1)/2

è®¡ç®—æœºè§†è§‰é‡Œfä¸€èˆ¬æ˜¯å¥‡æ•°ï¼Œå› ä¸º

1.å¦‚æžœfæ˜¯å¶æ•°ï¼Œåªèƒ½è¿›è¡Œä¸å¯¹ç§°çš„å¡«å……

2.å¥‡æ•°ç»´filteræœ‰ä¸€ä¸ªä¸­å¿ƒç‚¹ï¼Œä¾¿äºŽæŒ‡å‡ºè¿‡æ»¤å™¨çš„ä½ç½®

## å·ç§¯æ­¥é•¿

nÃ—n image    fÃ—f filter

padding p       stride s

è¾“å‡ºä¸º$\lfloor (n+2p-f)/s+1\rfloor Ã— \lfloor (n+2p-f)/s+1\rfloor$

cross-correlation vs. convolution

![conv](img/conv.png)

## å·ç§¯ä¸ºä½•æœ‰æ•ˆ

![rgb](img/rgb.png)

åªæ£€æµ‹çº¢è‰²çš„åž‚ç›´è¾¹ç¼˜

æ£€æµ‹æ‰€æœ‰é¢œè‰²çš„åž‚ç›´è¾¹ç¼˜

å¤šä¸ªè¿‡æ»¤å™¨

## å•å±‚å·ç§¯ç½‘ç»œ

![layer](img/layer.png)

![notation](img/notation.png)

## ç®€å•å·ç§¯ç½‘ç»œç¤ºä¾‹

channel(depth)è¶Šæ¥è¶Šå¤§ï¼ŒWidthå’ŒHeightè¶Šæ¥è¶Šå°

Types of layer in a convolutional network

Convolution

Pooling

Fully connected

## æ± åŒ–å±‚

Max pooling, Average pooling

Hyperparameters:

fï¼šfilter size

sï¼šstride

$n_HÃ—n_WÃ—n_Câ€”â€”>\lfloor \frac{n_H-f}s+1\rfloor Ã— \lfloor \frac{n_W-f}s+1\rfloorÃ—n_H$

No parameters to learnï¼

## å·ç§¯ç¥žç»ç½‘ç»œç¤ºä¾‹

![lenet1](img/lenet1.png)

![lenet2](img/lenet2.png)

## ä¸ºä»€ä¹ˆä½¿ç”¨å·ç§¯

å‚æ•°å…±äº«å’Œç¨€ç–è¿žæŽ¥

Parameter sharing: A feature detector (such as a vertical edge detector) that's useful in one part of the image is probably useful in another part of the image.

Sparsity of connections: In each layer, each output value depends only on a small number of inputs.

Use gradient descent to optimize parameters to reduce $J$

# Deep convolutional models: case studies 

## Case studies

### ä¸ºä»€ä¹ˆè¦è¿›è¡Œå®žä¾‹æŽ¢ç©¶

### ç»å…¸ç½‘ç»œ

**LeNet-5**

![lenet5](img/lenet5.png)

**AlexNet**

![alexnet](img/alexnet.png)

**VGG-16**

![vgg16](img/vgg16.png)

### æ®‹å·®ç½‘ç»œ

Residual Nerworks(ResNets)

Residual block

![resnets1](img/resnets1.png)

![resnets2](img/resnets2.png)

### æ®‹å·®ç½‘ç»œä¸ºä»€ä¹ˆæœ‰ç”¨

æ®‹å·®å—å­¦ä¹ æ’ç­‰å‡½æ•°éžå¸¸å®¹æ˜“ï¼Œæ•ˆçŽ‡å¹¶ä¸æ¯”ä¸€èˆ¬ç½‘ç»œä½Ž

![whyresnets](img/whyresnets.png)

### ç½‘ç»œä¸­çš„ç½‘ç»œä»¥åŠ1Ã—1å·ç§¯

![1by1](img/1by1.png)

### è°·æ­ŒInceptionç½‘ç»œ

ä¸éœ€è¦äººå·¥é€‰æ‹©è¿‡æ»¤å™¨ç»´åº¦ï¼ŒæŠŠæ‰€æœ‰å¯èƒ½æ‰”ç»™ä»–ï¼Œè‡ªå·±è®­ç»ƒ

![inception](img/inception.png)

![inception1](img/inception1.png)

![inception2](img/inception2.png)

### Inceptionç½‘ç»œ

å³GoogleNet

![inception3](img/inception3.png)

![inception4](img/inception4.png)

## Practical advices for using ConvNets

### ä½¿ç”¨å¼€æºçš„å®žçŽ°æ–¹æ¡ˆ

Github

### è¿ç§»å­¦ä¹ 

æ•°æ®ä¸å¤Ÿï¼Œä½¿ç”¨ç±»ä¼¼çš„ç¥žç»ç½‘ç»œçš„ä»£ç å’Œæƒé‡ï¼Œåˆ›é€ è‡ªå·±çš„è¾“å‡ºå±‚ï¼Œè®­ç»ƒè¾“å‡ºå±‚æƒé‡

è¿™æ—¶å¯ä»¥æå‰è®¡ç®—å¥½è¾“å‡ºå±‚ä¹‹å‰çš„æ¿€æ´»å‡½æ•°çš„è¾“å‡ºï¼Œå­˜åˆ°ç¡¬ç›˜ã€‚è¿™æ ·ä¹‹åŽå†è®­ç»ƒå°±å¯ä»¥å‡å°‘è®¡ç®—æˆæœ¬

ä¹Ÿå¯ä»¥é‡æ–°è®­ç»ƒå¤šå±‚çš„æƒé‡ï¼Œæ•°æ®é‡å¤Ÿçš„æƒ…å†µä¸‹ã€‚

å¦‚æžœæ•°æ®è¶³å¤Ÿå¤šï¼Œè®­ç»ƒæ‰€æœ‰çš„æƒé‡ï¼Œå°†ä¸‹è½½çš„æƒé‡ä½œä¸ºåˆå§‹åŒ–æƒé‡

### æ•°æ®æ‰©å……

Mirroringé•œåƒ, Random Croppingéšæœºè£å‰ª, Rotationæ—‹è½¬, Shearingå‰ªåˆ‡, Local warpingå±€éƒ¨å¼¯æ›²

Color shiftingé¢œè‰²å˜æ¢

Advanced:   PCA, AlexNet paper "PCA color augumentation"



### è®¡ç®—æœºè§†è§‰çŽ°çŠ¶

Two sources of knowledge

- Labeled data
- Hand engineered features/network architecture/other components

æ•°æ®é‡å°‘ï¼Œæ›´å¤šçš„è¿˜æ˜¯ç”¨æ‰‹å·¥ç‰¹å¾å·¥ç¨‹

Tips for doing well on benchmarks/winning competitions

Ensembling

- Train several networks independently and average their outputs

Multi-crop at test time

- Run classifier on multiple versions of test images and average results
- 10-cropï¼šå¯¹å›¾åƒåŠé•œåƒå–ä¸­é—´çš„ã€å››ä¸ªè§’çš„å›¾è¿›è¡Œè®­ç»ƒ

Use open source code

- Use architectures of networks published in the literature
- Use open source implementations if possible
- Use pretrained models and fine-tune on your dataset

#Object detection algorithms

## ç›®æ ‡å®šä½

ä¸ä»…è¦åˆ¤æ–­å›¾ç‰‡ç‰©ä½“æ˜¯ä»€ä¹ˆï¼Œè¿˜è¦åˆ¤æ–­å…¶å…·ä½“ä½ç½®

é™¤äº†ç‰©ä½“ç±»åˆ«ï¼Œè¿˜è¦è®©ç¥žç»ç½‘ç»œè¾“å‡ºç‰©ä½“çš„è¾¹æ¡†çš„å››ä¸ªæ•°å­—ï¼Œ(bx, by, bh, bw)ç‰©ä½“çš„ä¸­å¿ƒç‚¹ä½ç½®ï¼Œé«˜åº¦ï¼Œå®½åº¦

![objdetect1](img/objdetect1.png)

## ç‰¹å¾ç‚¹æ£€æµ‹

landmark detection

è¾“å‡ºæ¯ä¸ªç‰¹å¾ç‚¹çš„åæ ‡ï¼Œæ¯”å¦‚äººè„¸è¯†åˆ«ä¸­çš„çœ¼ç›ã€å˜´å·´ã€å¾®ç¬‘ç­‰è„¸éƒ¨ç‰¹å¾

äººä½“å§¿æ€æ£€æµ‹ï¼Œè®¾å®šå…³é”®ç‰¹å¾ç‚¹ï¼Œæ ‡æ³¨ï¼Œè®­ç»ƒï¼Œç¡®å®šäººç‰©å§¿æ€åŠ¨ä½œ

## ç›®æ ‡æ£€æµ‹

Sliding windows detectionæ»‘åŠ¨çª—å£ç›®æ ‡æ£€æµ‹æ–¹æ³•

ä»¥å›ºå®šå¹…åº¦æ»‘åŠ¨çª—å£ï¼ŒéåŽ†å›¾åƒçš„æ¯ä¸ªåŒºåŸŸï¼ŒæŠŠè¿™äº›å‰ªåˆ‡åŽçš„å°å›¾åƒè¾“å…¥å·ç§¯ç½‘ç»œï¼Œç„¶åŽè¿›è¡Œåˆ†ç±»

é‡å¤å¤šæ¬¡ï¼Œæ‰©å¤§çª—å£å¤§å°

æ­¥å¹…å°â€”â€”æˆæœ¬é«˜ï¼Œæ­¥å¹…å¤§â€”â€”æ€§èƒ½ä½Ž

## å·ç§¯çš„æ»‘åŠ¨çª—å£å®žçŽ°

![objdetect2](img/objdetect2.png)

![objdetect3](img/objdetect3.png)

## Bounding Boxé¢„æµ‹

YOLO algorithm

å°†å›¾åƒåˆ†æˆå¾ˆå¤šä¸ªæ ¼å­ï¼Œå¯¹æ¯ä¸ªæ ¼å­ï¼Œéƒ½æœ‰ä¸€ä¸ªæ ‡ç­¾yï¼Œå°±æ˜¯ä¸Šé¢ç›®æ ‡å®šä½ä¸­çš„y

ä¸æ˜¯å¯¹æ¯ä¸ªæ ¼å­åˆ†åˆ«è¿è¡Œç®—æ³•ï¼Œè€Œæ˜¯åˆ©ç”¨å·ç§¯ä¸€æ¬¡å®žçŽ°ï¼Œæ‰€ä»¥è¿è¡Œé€Ÿåº¦å¾ˆå¿«ï¼Œç»å¸¸ç”¨äºŽå®žæ—¶æ£€æµ‹

![objdetect4](img/objdetect4.png)

![objdetect5](img/objdetect5.png)

## äº¤å¹¶æ¯”

Intersection over union(IoU)

= size of intersection/size of union

å…¶ä¸­intersectionå’Œunionæ˜¯é¢„æµ‹è¾¹æ¡†ä¸ŽçœŸå®žè¾¹æ¡†çš„äº¤é›†å’Œå¹¶é›†

Correct if IoU>=0.5

## éžæžå¤§å€¼æŠ‘åˆ¶

non-max suppression

![objdetect6](img/objdetect6.png)

## Anchor Boxes

![anchor1](img/anchor1.png)

![anchor2](img/anchor2.png)

![anchor3](img/anchor3.png)

## YOLOç®—æ³•

putting it together: YOLO algorithm

![yolo1](img/yolo1.png)

![yolo2](img/yolo2.png)

![yolo3](img/yolo3.png)

## RPNç½‘ç»œ

![rcnn](img/rcnn.png)

## ç›®æ ‡æ£€æµ‹ç®—æ³•å®žè·µ(è¡¥å……)

å‚è€ƒhttps://mp.weixin.qq.com/s/2mIuy4t3QUdSo0xaBuJRXA

https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9

# Special applications: Face recognition & Neural style transfer 

## Face Recognition

### ä»€ä¹ˆæ˜¯äººè„¸è¯†åˆ«

Face verification vs. face recognition

Verification

- Input image, name/ID
- Output whether the input image is that of the claimed person

Recognition

- Has a database of K persons
- Get an input image
- Output ID if the image is any of the K persons (or â€œnot recognizedâ€)

### One-shot learning

æ ¹æ®ä¸€å¼ ç…§ç‰‡å­¦ä¹ 

Learning from one example to recognize the person again

Learning a "similarity" function

d(img1, img2) = degree of difference between images

If d(img1, img2) â‰¤ Ï„ï¼Œ "same"     else  "different"

### Siameseç½‘ç»œ

ç”¨åŒæ ·çš„å·ç§¯ç¥žç»ç½‘ç»œæž¶æž„å’Œå‚æ•°ï¼Œä¸¤ä¸ªimgä½œä¸ºè¾“å…¥ï¼Œå¾—åˆ°ä¸¤ä¸ªimgçš„encodingï¼Œæ¯”è¾ƒä¸¤ä¸ªencodingçš„è·ç¦»

Goal of learning

Parameters of NN define an encoding $f(x^{(i)})$

Learn parameters so that:

â€‹	If $x^{(i)}, x^{(j)}$ are the same person, $||f(x^{(i)})-f(x^{(j)})||^2$ is small.

â€‹	If $x^{(i)}, x^{(j)}$ are the different person, $||f(x^{(i)})-f(x^{(j)})||^2$ is large.

[Taigman et. al., 2014. DeepFace closing the gap to human level performance]

### Triplet æŸå¤±

ä¸ºäº†é˜»æ­¢å­¦ä¹ åˆ°çš„æ‰€æœ‰ç¼–ç éƒ½æ˜¯ä¸€æ ·çš„ï¼Œè¦åŠ ä¸€ä¸ªmarginï¼Œå¹¶ä¸”æ‹‰å¤§äº†æ­£è´Ÿæ ·æœ¬ä¸¤è€…ä¹‹é—´çš„è·ç¦»

![face1](img/face1.png)

![face2](img/face2.png)

![face3](img/face3.png)



å› ä¸ºè¦æ±‚çš„è®­ç»ƒé›†å¾ˆå¤§ï¼Œæˆæœ¬è¾ƒé«˜ï¼Œæ‰€ä»¥ä¸å¿…ä»Žå¤´å¼€å§‹ï¼Œå¯ä»¥ä¸‹è½½åˆ«äººçš„é¢„è®­ç»ƒæ¨¡åž‹

è®ºæ–‡ï¼šA unified embedding for face recognition and clustering

### é¢éƒ¨éªŒè¯ä¸ŽäºŒåˆ†ç±»

æœ€åŽä¸€å±‚è¿›è¡ŒäºŒåˆ†ç±»ï¼ŒåŒä¸€ä¸ªäººè¾“å‡º1ï¼Œå¦åˆ™è¾“å‡º0

![face4](img/face4.png)



##Neural Style Transfer

### ä»€ä¹ˆæ˜¯ç¥žç»é£Žæ ¼è¿ç§»

![nst1](img/nst1.png)

###æ·±åº¦å·ç§¯ç½‘ç»œåœ¨å­¦ä»€ä¹ˆ

![nst2](img/nst2.png)



é åŽçš„éšè—å•å…ƒä¼šçœ‹åˆ°æ›´å¤§çš„å›¾ç‰‡å—

![nst3](img/nst3.png)

![nst4](img/nst4.png)

![nst5](img/nst5.png)

![nst6](img/nst6.png)

[Zeiler and Fergus., 2013, Visualizing understanding convolutional networks]

### ä»£ä»·å‡½æ•°

![nst7](img/nst7.png)

![nst8](img/nst8.png)

### å†…å®¹ä»£ä»·å‡½æ•°

![nst9](img/nst9.png)

### é£Žæ ¼æŸå¤±å‡½æ•°

Say you are using layer ð‘™â€™s activation to measure â€œstyle.â€
Define style as correlation between activations across channels.

How correlated are the activations across different channels?

ç›¸å…³ç³»æ•°æè¿°äº†è¿™äº›ä¸åŒç‰¹å¾åŒæ—¶å‡ºçŽ°çš„æ¦‚çŽ‡

![nst10](img/nst10.png)

![nst11](img/nst11.png)

![nst12](img/nst12.png)

### ä¸€ç»´åˆ°ä¸‰ç»´å·ç§¯æŽ¨å¹¿

14Ã—1   *  5Ã—1 â€”â€”> 10Ã—16 (16 filters)   * 5Ã—16â€”â€”>  6Ã—32  (32filters)

14Ã—14Ã—14Ã—1   * 5Ã—5Ã—5Ã—1  â€”â€”>10Ã—10Ã—10Ã—16 ï¼ˆ16 filters)   *  5Ã—5Ã—5Ã—16 â€”â€”> 6Ã—6Ã—6Ã—32   (32filters)



