# 循环序列模型

## 为什么选择序列模型

![sequence](img/sequence.png)

## notation

![seqnotation](img/seqnotation.png)

## 循环神经网络模型

Recurrent Neural Networks

Why not a standard network?

Problems:

- Inputs, outputs can be different lengths in different examples.比如每个句子长度不一样
- Doesn't share features learned across different positions of text.比如每个名字在每个句子中出现的位置不一样

![rnn1](img/rnn1.png)

缺点：没有用到序列后面的信息

![rnn2](img/rnn2.png)

![rnn3](img/rnn3.png)

## Backpropagation through time

![rnn4](img/rnn4.png)

## 不同类型的RNN

![rnn5](img/rnn5.png)

举例

One to many:  Music generation

Many to one: Sentiment classification

Many to many: 命名实体识别

Many to many: Machine translation，前半部分是encoder,后半部分是decoder

## 语言模型和序列生成

![rnn6](img/rnn6.png)

![rnn7](img/rnn7.png)

corpus:很长的或数量众多的英文句子组成的文本

Tokenize标记化:比如给定字典，把每个单词变成one-hot向量

EOS: end of sentence

UNK：unknown word，不在字典中

![rnn8](img/rnn8.png)

## 对新序列采样

![rnnsample](img/rnnsample.png)

![rnnchar](img/rnnchar.png)

![rnnseq](img/rnnseq.png)

## Vanishing gradient with RNNs

![rnnvanish](img/rnnvanish.png)

梯度爆炸问题可以通过梯度修剪gradient clipping解决

## GRU单元

![rnngru1](img/rnngru1.png)

![rnngru2](img/rnngru2.png)

![rnngru3](img/rnngru3.png)

## LSTM

![rnnlstm](img/rnnlstm.png)

![rnnlstm2](img/rnnlstm2.png)

## 双向RNN

Bidirectional RNN

![rnnb1](img/rnnb1.png)

![rnnb2](img/rnnb2.png)

## Deep RNNs

![rnndeep](img/rnndeep.png)

# 自然语言处理与词嵌入

## 词嵌入简介

### 词汇表征

1-hot representation：把每个词孤立起来，很难泛化两个单词之间的关系

featurized representation：

![5rep](img/5rep.png)

visualizing word embeddings:  t-SNE

### 使用词嵌入

会使用大量文本，迁移学习到命名实体识别

Transfer learning and word embeddings:

1. Learn word embeddings from large text corpus.(1-100B words)

   (Or download pre-trained embedding online.)

2. Transfer embedding to new task with smaller training set.(say, 100k words)

3. Optional: Continue to finetune the word embeddings with new data.

Relation to face encoding:

encoding和embedding类似

但是encoding可以对一个未知图片学习encoding, embedding是对已有的词汇学习出每个词的embedding

### 词嵌入的特性

Analogies

e(man) - e(woman) ≈ e(king) - e(queen)

Mikolov et. al., 2013, Linguistic regularities in continuous space word representations

For word w： argmax sim(e(w), e(king) - e(man)+e(woman))

Cosine similarity

![5cosine](img/5cosine.png)

### 嵌入矩阵

embedding matrix

每一列代表词的向量，行代表向量，维度是：向量维度*词数量

embedding matrix(E) × word(j)'s one-hot = word(j)'s embedding

In practice, use specialized function to look up an embedding.

## 学习词嵌入：Word2vec & Glove

### 学习词嵌入

Neural language model

![5nerual](img/5nerual.png)

![5context](img/5context.png)

可以看所有单词，也可以只看前面几个单词，固定的窗口

Bengio et. al., 2003, A neural probabilistic language model

### Word2vec

Skip-grams

在句子中随机选择一个词作为context word,再在这个词的前后比如5个间距的单词里随机选择一个作为Target word.

比如句子：I want a glass of orange juice to go along with my cereal.

context :    orange	orange	orange

Target   :    juice		glass	my

这样就形成三组词，context word是输入x, target word是输出y

首先根据词嵌入矩阵×单词的one-hot向量得到输入词c的词嵌入$e_c$, 经过softmax层，得到输出$\hat y$

$softmax:  p(t|c) = e^(\theta_t^T e_c) / \sum_{j=1}^{10000} e^(\theta_j^T e_c), 其中 \theta_t 是与输出t有关的参数$

$Loss(\hat y, y)= -\sum_{i=1}^{10000}y_ilog\hat y_i， y是一个one-hot 向量$

Problems with softmax classification

一、计算softmax的分母，计算速度慢

solutions:

1. Hierarchical softmax，多个二分类器组合成一棵树，比如第一次判断这个词是在词汇表的前5000个还是后5000个，第二次判断是在前5000个词的前2500个，还是后2500个，以此类推。

常用词放在前面，不常用的词放在底部，树最深的地方，复杂度大约是log|V|

2. 负采样

二、How to sample the context c？

如果使用均匀采样，常见词比如the of 等会常常被采样到，重要词很少被采样到。实际中会使用一些启发式的策略进行采样。

Mikolov rt. al., 2013, Efficient estimation of word representations in vector space.

### 负采样

抽取context word和target word形成正样本，标记为1；在词典中随机选择一个词和context word组成负样本，标记为0。k个负样本

输入x变成一对词(c,t)，输出y是对应的0或1

$P(y=1 | c, t) = \sigma (\theta_t^T e_c)$ 使用多个二分类器如logistic，每次只训练选择的正样本和负样本。

selecting negative examples

$p(w_i) = f(w_i)^{\frac34} / \sum_{j=1}^{10000} f(w_j)^{\frac34}，其中f(w_i)是单词w_i的词频$

Mikolov rt. al., 2013, Distributed representation of words and phrases and their compositionality

### Glove词向量

global vectors for word representation

(c, t)

$X_{ij}$ = times i appears in context of j

minimize$  \sum_{i=1}^{10000}\sum_{j=1}^{10000}f(X_{ij})(\theta_i^T e_j + b_i + b_j - logX_{ij})^2$

weighting term $f(X_{ij}) = 0 如果 x_{ij}=0，且定义0log0=0, 且对于停用词给一个不过分的权重，以及不常用的词一个不小的有意义的权重$

$\theta_i和e_j是对称的，所以可以有e_w^{(final)} = \frac{e_w+\theta_w}2$

注意word embeddings的每个轴不一定是有解释性的。

Pennington et. al., 2014. Golve: Global vectors for word representation

## 词嵌入的应用

### 情绪分类

经常没有足够的标记样本

比如评论对应打分(1-5分)

1. Simple sentiment classification model

把所有单词的词嵌入加起来或者平均，输入给softmax，输出1-5的分类

词嵌入的学习来自其它大量样本

问题：没有考虑词序，比如某个词good出现了很多次，最后得到的结果会是高分，但实际上是个差评：

Completely lacking in good taste, good service, and good ambience.

2. RNN for sentiment classification

many-to-one， 输入是单词的embedding, 输出是1-5的分类

### 词嵌入除偏

Debiasing word embeddings

![5debias](img/5debias.png)

![5debias2](img/5debias2.png)

not definitional比如doctor,babysitter等

Equalize pairs，使得babysitter和grandmother的距离, babysitter和grandfather的距离两者相近，没有显著区别

Bolukbasi rt. al. , 2016, Man is to computer as woman is to homemaker? Debiasing word embeddings

# 序列模型和注意力机制

## 多种sequence to sequence architectures

### 基础模型

![5seq2seq](img/5seq2seq.png)

![5image](img/5image.png)

### 选择最可能的句子

![5trans](img/5trans.png)

![5trans2](img/5trans2.png)

![5trans3](img/5trans3.png)

贪心搜索：先找到第一个最好的，然后找第二个单词最好的，以此类推

### 定向搜索

Beam search集束搜索

第一步：选择翻译的第一个单词，通过RNN，softmax输出概率，选择前B个单词保留下来，并且记录概率

第二步：将第一步得到的单词作为输入，分别训练，得到第二个单词的概率，选择前B个单词，并记录概率

以此类推， 直到出现句子结尾标志。

![5beam1](img/5beam1.png)

![5beam2](img/5beam2.png)

![5beam3](img/5beam3.png)

### 改进定向搜索

概率相乘，容易产生数值下溢underflow，所以取log

另外，由于概率小于1，取log后小于0，翻译会倾向于选择简短的句子，这样概率值会大一些

所以要除以句子长度

![5beam4](img/5beam4.png)

Beam search discussion

large B: better result, slower

small B: worse result, faster

Beam width B: B越大，性能提升越不明显。B的选择取决于应用。一般可以取10，100(蛮大了),工业中也可能取1000，3000等。

Unlike exact 精确search algorithms like BFS (Breadth First Search) or DFS(Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for $argmax_y P(y|x)$

### 定向搜索的误差分析

![5error1](img/5error1.png)

![5error2](img/5error2.png)

![5error3](img/5error3.png)

### Bleu得分

![5bleu1](img/5bleu1.png)

![5bleu2](img/5bleu2.png)

![5bleu3](img/5bleu3.png)

![5bleu4](img/5bleu4.png)

### 注意力模型直观理解

神经网络不擅于处理长句子翻译

生成第一个词的时候，对第一个词、第二个词...应该放多少注意力。

![5attention1](img/5attention1.png)

![5attention2](img/5attention2.png)

### 注意力模型

![5attention3](img/5attention3.png)

![5attention4](img/5attention4.png)

![5attention5](img/5attention5.png)

## Speech recognition-Audio data

### 语音辨识

![5speech1](img/5speech1.png)

![5speech2](img/5speech2.png)

![5speech3](img/5speech3.png)

### 触发字检测

![5trigger1](img/5trigger1.png)

![5trigger2](img/5trigger2.png)

音频中发现trigger word的地方输出1，其它地方输出0.但是这样会导致0和1的比例很不均衡，可以采用一种粗暴的做法：将出现1的地方的后面若干个输出也变成1

## Conclusion

