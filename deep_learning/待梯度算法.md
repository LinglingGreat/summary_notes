**SGD, BGD, Adadelta, Momentum哪个方法对超参数最不敏感？**

神经网络经典五大超参数:
学习率(Learning Rate)、权值初始化(Weight Initialization)、网络层数(Layers)
单层神经元数(Units)、正则惩罚项（Regularizer|Normalization)

显然在这里超参数指的是事先指定的learningrate，而对超参数不敏感的梯度算法是Adadelta，牛顿法。
Adadelta自适应学习率调整
Adadelta的特点是在下降初期，梯度比较小，这时学习率会比较大，而到了中后期，接近最低点时，梯度较大，这时学习率也会相对减小，放慢速度，以便可以迭代到最低点。
Momentum冲量法
梯度下降法在求解时的难题主要在于解决极小值和鞍点的问题，为了解决这个问题，可以模拟在现实中的惯性。物体有一个初始动量，在平缓的区域或者小的坑里也能继续向前运动试图滚出小坑，在动量变为0的时候停止，表示已经达到最低点。
https://blog.csdn.net/qq_34470213/article/details/79869206